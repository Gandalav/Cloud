<!DOCTYPE html>
<!-- saved from url=(0035)https://theproject.zone/writeup/3/4 -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

    <link type="image/x-icon" rel="shortcut icon" href="https://theproject.zone/static/favicon.ico">
    <!--<link type="text/css" rel="stylesheet" href="/static/css/table_sort.css">-->
    <link type="text/css" rel="stylesheet" href="./P3.4_files/bootstrap.min.css">
    <link type="text/css" rel="stylesheet" href="./P3.4_files/jquery.dataTables.min.css">
    <link type="text/css" rel="stylesheet" href="./P3.4_files/header.css">
    <link type="text/css" rel="stylesheet" href="./P3.4_files/defaultTheme.css">
    <link type="text/css" rel="stylesheet" href="./P3.4_files/datepicker.css">
    <link type="text/css" rel="stylesheet" href="./P3.4_files/bootstrap-editable.css">
    <title>15-319/619 Cloud Computing</title>
    
    
    
    
    
    <link type="text/css" rel="stylesheet" href="./P3.4_files/inside.base.css">

    <link type="text/css" rel="stylesheet" href="./P3.4_files/docs.min.css">


    <script type="text/javascript" src="./P3.4_files/jquery.min.js"></script>
    <script type="text/javascript" src="./P3.4_files/jquery.tablesorter.js"></script>
    <script type="text/javascript" src="./P3.4_files/jquery.dataTables.min.js"></script>
    <script type="text/javascript" src="./P3.4_files/jquery.fixedheadertable.js"></script>
    <script type="text/javascript" src="./P3.4_files/fnSetFilteringDelay.js"></script>
    <script type="text/javascript" src="./P3.4_files/datatableview.min.js"></script>
    <script type="text/javascript" src="./P3.4_files/bootstrap.min.js"></script>
    <script type="text/javascript" src="./P3.4_files/bootstrap-datepicker.js"></script>
    <script type="text/javascript" src="./P3.4_files/bootstrap-editable.min.js"></script>
<link rel="stylesheet" type="text/css" href="./P3.4_files/prettify.css"></head>

<body role="document" style="margin-bottom: 40px">

    <div id="base-container" class="container-fluid" style="padding-top: 49.888888835907px;">
    
    
    
    
    
    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
   
        <div class="container-fluid">

            <div class="navbar-header" style="padding-bottom:40px">
               <a href="https://theproject.zone/home"><img src="./P3.4_files/TPZlogo_small_inverted.png"></a>
            </div>

            <div class="navbar-header pull-right">
                <a class="navbar-brand" href="https://theproject.zone/writeup/3/4#" onclick="return false;" style="margin-left: 0px;"><span class="label label-default" id="username-label">pgandala</span></a>
            </div>

            <div class="coll pull-right">
                <ul id="navbar" class="nav navbar-nav" style="font-weight:bold">
                    <li id="home"><a href="https://theproject.zone/home">Home</a></li>

                    
                        
                        <li id="project0" class="dropdown">
                            <a href="https://theproject.zone/writeup/3/4#" data-toggle="dropdown" class="dropdown-toggle">Primer
                            
                                <b class="caret"></b>
                            
                            </a>
                            
                            <ul class="dropdown-menu">
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/0/1">Project Primer</a></li>
                                        
                                    
                                
                            </ul>
                            
                        </li>
                        
                        <li id="project1" class="dropdown">
                            <a href="https://theproject.zone/writeup/3/4#" data-toggle="dropdown" class="dropdown-toggle">Project 1
                            
                                <b class="caret"></b>
                            
                            </a>
                            
                            <ul class="dropdown-menu">
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/1/1">1.1 Sequential Analysis</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/1/2">1.2 Using Amazon's Elastic MapReduce</a></li>
                                        
                                    
                                
                            </ul>
                            
                        </li>
                        
                        <li id="project2" class="dropdown">
                            <a href="https://theproject.zone/writeup/3/4#" data-toggle="dropdown" class="dropdown-toggle">Project 2
                            
                                <b class="caret"></b>
                            
                            </a>
                            
                            <ul class="dropdown-menu">
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/2/1">2.1 Introduction to AWS APIs</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/2/2">2.2 Load Balancing and AutoScaling</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/2/3">2.3 Advanced Scaling Concepts</a></li>
                                        
                                    
                                
                            </ul>
                            
                        </li>
                        
                        <li id="project3" class="dropdown active">
                            <a href="https://theproject.zone/writeup/3/4#" data-toggle="dropdown" class="dropdown-toggle">Project 3
                            
                                <b class="caret"></b>
                            
                            </a>
                            
                            <ul class="dropdown-menu">
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/3/1">3.1 Files v/s Databases</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/3/2">3.2 Partitioning and Replication</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/3/3">3.3 Database-as-a-Service</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="./P3.4_files/P3.4.html">3.4 Cloud Data Warehousing</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/3/5">3.5 Consistency in Distributed Key-Value Stores</a></li>
                                        
                                    
                                
                            </ul>
                            
                        </li>
                        
                        <li id="project4" class="dropdown">
                            <a href="https://theproject.zone/writeup/3/4#" data-toggle="dropdown" class="dropdown-toggle">Project 4
                            
                                <b class="caret"></b>
                            
                            </a>
                            
                            <ul class="dropdown-menu">
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/4/1">4.1 MapReduce Programming using YARN</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/4/2">4.2 Iterative Programming using Apache Spark</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/4/3">4.3 Graph Programming using GraphLab</a></li>
                                        
                                    
                                
                            </ul>
                            
                        </li>
                        
                        <li id="project5" class="dropdown">
                            <a href="https://theproject.zone/writeup/3/4#" data-toggle="dropdown" class="dropdown-toggle">15619 Project
                            
                                <b class="caret"></b>
                            
                            </a>
                            
                            <ul class="dropdown-menu">
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/5/1">Phase 1</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/5/2">Phase 1 Report</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/5/3">Phase 2</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/5/4">Phase 2 Live Test (MySQL)</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/5/5">Phase 2 Live Test (HBase)</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/5/6">Phase 2 Report</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/5/7">Phase 3</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/5/9">Phase 3 Live Test</a></li>
                                        
                                    
                                
                                    
                                        
                                        
                                        
                                        <li><a class="active" href="https://theproject.zone/writeup/5/10">Phase 3 Report</a></li>
                                        
                                    
                                
                            </ul>
                            
                        </li>
                        
                    
                
                    
                    <li id="grade"><a class="active" href="https://theproject.zone/grade/">Grade Book</a></li>
                    <li id="profile"><a href="https://theproject.zone/profile">Profile</a></li>
                    <li id="logout"><a href="https://theproject.zone/log_out">Log Out</a></li>
                </ul>
            </div>
        </div>
    </nav>


    <div class="container">

        <div class="row">

        <div class="col-md-6">
            <h2>3.4 Cloud Data Warehousing</h2>
        </div>
        
        
        <div class="col-md-6 pull-down">
            <div class="btn-group pull-right" style="margin-bottom: 10px">
                <a class="btn btn-default btn-primary" href="https://theproject.zone/writeup/3/4/" id="nav_writeup">Writeup</a>

                

                

                <a class="btn btn-default" href="https://theproject.zone/submissions/3/4/" id="nav_submissions">Submissions</a>
                <a class="btn btn-default" href="https://theproject.zone/scoreboard/3/4/" id="nav_score_board">ScoreBoard</a>
            </div>
        </div>
        
        
        </div>

        <table class="table table-striped table-bordered table-condensed prettyBorder">
        <thead>
            <tr>
                <th>Phase</th>
                <th>Open</th>
                <th>Deadline</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>3.4 Cloud Data Warehousing</td>
                <td>Mar. 22, 2015 00:01</td>
                <td>Mar. 29, 2015 23:59</td>
            </tr>
        </tbody>
        </table>
        
        

        <hr>
        

<style>
  img{
  display: block;
  margin-left: auto;
  margin-right: auto;
  margin-bottom: 10px;
  }
  small.caption {
  display: block;
  text-align: center;
  margin-bottom: 4em;
  }
  iframe {
  display: block;
  margin-left: auto;
  margin-right: auto;
  margin-bottom: 30px;
  }
  .bs-callout-task {
  background-color: #ffffff;
  border-left-color: green;
  }
  .bs-callout-task h4,
  .bs-callout-task a.alert-link {
  color: green;
  }
  .bs-callout-learning {
  background-color: #ffffff;
  border-left-color: purple;
  }
  .bs-callout-learning h4,
  .bs-callout-learning a.alert-link {
  color: purple;
  }
  .bs-docs-sidebar.affix {
  position: fixed;
  top: 25x;
  }
</style>

<div id="writeup-content" class="row">
  <div class="col-md-3" role="complementary">
    <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix-top" style="top:120px">
      <!-- Inline CSS Hack for Display -->
      <ul class="nav bs-docs-sidenav">
        <li><a href="https://theproject.zone/writeup/3/4#intro">Introduction</a>
            <ul class="nav">
            <li><a href="https://theproject.zone/writeup/3/4#data">Data Warehouses</a></li>
            <li><a href="https://theproject.zone/writeup/3/4#oltp">OLTP vs OLAP</a></li>
            <li><a href="https://theproject.zone/writeup/3/4#the-scenario">The Scenario</a></li>
            </ul>
        </li>
        <li>
          <a href="https://theproject.zone/writeup/3/4#star">The Star Schema Benchmark Schema Design</a>
          <ul class="nav">
          </ul>
        </li>
        <li>
          <a href="https://theproject.zone/writeup/3/4#warehouse">Introduction to MySQL, Hive and Redshift</a>
          <ul class="nav">
            <li><a href="https://theproject.zone/writeup/3/4#mysql">MySQL</a></li>
            <li><a href="https://theproject.zone/writeup/3/4#hive">Hive</a></li> 
            <li><a href="https://theproject.zone/writeup/3/4#redshift">Redshift</a></li>    
          </ul>
        </li>
         <li>
          <a href="https://theproject.zone/writeup/3/4#bench">Benchmarking</a>
          <ul class="nav">
            <li><a href="https://theproject.zone/writeup/3/4#mysql_task">MySQL Task</a></li>
            <li><a href="https://theproject.zone/writeup/3/4#hive_task">Hive Task</a></li>
            <li><a href="https://theproject.zone/writeup/3/4#redshift_task">Redshift Task</a></li>
          </ul>
        </li>
      </ul>
      <a class="back-to-top" href="https://theproject.zone/writeup/3/4#top">
      Back to top
      </a>
    </nav>
  </div>
<div class="col-md-9" role="main">
<div class="bs-docs-section">
<h1 id="intro" class="page-header">Introduction</h1>
<div class="bs-callout bs-callout-learning">
<h4>Learning Objectives</h4>
<p>This project will encompass the following learning objectives:</p>
<ol>
<li>Compare the design of Online Transaction Processing (OLTP) with Online Analytic Processing (OLAP) systems and the role of OLAP in Business Intelligence systems.</li>
<li>Analyze the advantages and disadvantages of using the MapReduce programming model for data warehousing by running a typical Business Intelligence query using Hive.</li>
<li>Compare the differences between traditional row stores and distributed columnar stores by implementing optimizations in column stores for Business Intelligence queries.</li>
<li>Design an optimized table structure with sort keys and distribution keys for leveraging parallel processing in massively parallel systems like Redshift. </li>
</ol>
</div>
<h3 id="data">Data Warehouses</h3>
<p>A data warehouse is a copy of transaction data specifically structured for querying and reporting. Decision Support Systems (DSS) help organizations in strategic and operational decision making processes. Strategic decision making requires historical data. Data Warehouses(DW) are large storage systems which store historical data that can be used for strategic decision making. </p>
<p>Day to day business operations (transactions) which require high throughput typically use OLTP (Online Transaction Processing) systems whereas for business intelligence systems (data warehouse) OLAP (Online Analytical Processing) is used. The following section gives more details on OLTP and OLAP.</p>
<h3 id="oltp">OLTP vs OLAP</h3>
<p>An OLTP (Online Transaction Processing) System deals with operational data, which is, data involved in the operation of a particular system.</p>
<p>OLTP is characterized by a large number of short online transactions (INSERT, UPDATE, DELETE). In an OLTP system data are frequently updated  and queried. So a fast response to a request is required. Since OLTP systems involve a large number of update queries, the database tables are optimized for write operations.</p>
<p>To prevent data redundancy and to prevent update anomalies the database tables are normalized. The process of organizing the attributes of a table in a relational database is called normalization. Normalization makes the write operation in the database tables more efficient because the total data written to disk on a transaction is reduced. The set of tables that are normalized are also fragmented. The fragmentation of tables makes reading large amount of data for complex queries inefficient as data from same table is stored together and reading from across different tables incurs heavy disk latency.</p>
<p>Operational data are usually of local relevance. It involves queries accessing individual tuples (an individual record). These types of queries are termed as point queries.</p>
<p>Example OLTP query: What is the Salary of Mr.John?</p>
<p>OLAP (Online Analytical Processing) deals with historical data or archival data. Historical data are those data that are archived over a long period of time. Data from OLTP are collected over a period of time and stored in a very large database called a Data Warehouse. Data warehouses are highly optimized for read (SELECT) operation.</p>
<p>OLAP queries are of analytical form, meaning that they need to access a large amount of data and require many aggregations. It accesses a large number of records from database tables and performs filtering and aggregations on the required columns.</p>
<p>Updates are very rare in a Data warehouse. OLAP queries enable strategic decision making through enabling the analysis of historical data.</p>
<p>An example OLAP query: How is the profit changing over the last several months across different regions?</p>
<img src="./P3.4_files/3_4_feedback_loop.png" width="90%" height="50%">
<h4>
  <small class="caption">
    <p>Figure 1: Feedback loop in a Business Intelligence system</p>
  </small>
</h4>
<h3 id="the-scenario">The Scenario</h3>
<p>Carnegie Records(CR) has become very successful with sales in more than a 100 countries and half a billion customers worldwide. They would like to expand their business even further and for that reason, they want to analyze their historical sales data which has several hundred million records. Since you have been very successful at your job of scaling and maintaining their transaction processing system, they have asked you to develop their analytics and reporting workflow using the latest technologies available in the market today.</p>
<p>Since this is the first time Carnegie Records is building an analytics and reporting framework for their decision support system the CTO of CR has asked you to benchmark multiple Business Intelligence (BI) tools so that they use the best system available for their workload. The BI tool will help them make strategic decisions for historical sales data. The executive team from CR wants answers to questions like,
</p><ol>
<li>What is the increase in revenue if discounts are reduced for a given time period</li>
<li>What is the revenue from a certain product for a certain region</li>
</ol>
<p></p>
<p>Using the insights gained from the historical data captured from their transaction processing system, CR hopes to improve its business. This will be accomplished by taking well informed strategic decisions for various aspects of their business, such as promoting certain products in certain regions, applying discounts to products etc. Figure 1 shows the feedback loop that portrays businesses using Business Intelligence systems.</p>
<p>After a literature survey, you have decided to use the Star Schema Benchmark (SSB). The SSB is designed to measure the performance of database products with respect to data-warehousing workloads like reporting and analytics. The design principles and query design of the star schema benchmark are explained in the next section.</p>
<img src="./P3.4_files/3_4_task_overview.png" width="90%" height="50%">
<h4>
  <small class="caption">
    <p>Figure 2: Project 3.4 Task Overview</p>
  </small>
</h4>
<p>You will be doing the following tasks in this project,
</p><ol>
<li>Provision a MySQL instance with the star schema benchmark data-set and benchmark the performance of query1 in ssb. This task will help you analyze the design of a typical row-store OLTP system.</li>
<li>Provision a Hive (EMR) Cluster. Create an external table in Hive that reads from S3 and benchmark the performance of query1 in ssb. This task will help you analyze the advantages and disadvantages of using MapReduce programming model for data warehousing workloads.</li>
<li>Provision a Redshift cluster, load ssb data-set to structured (relational) tables and benchmark queries 1, 2 and 3 in ssb. This task will help you compare the performance of a traditional row store OLTP system (MySQL) with a distributed columnar store (Redshift).</li>
<li>Optimize the table structure in Redshift using sort keys and dist keys in order to improve the performance of the ssb queries by leveraging the parallel execution and columnar compression in distributed columnar stores.</li>
</ol>
<p></p>
<p>The MySQL instance, Hive Cluster and the Redshift Cluster are the backend in the context of this project.</p>
<p>The following section will give a brief introduction to the SSB benchmark schema design and the queries of the benchmark. This will be followed by an introduction to MySQL, Hive and Redshift as well as their design principles. Finally we will go through the detailed steps for the tasks mentioned above.</p>
<div class="bs-callout bs-callout-warning">
<h4 id="resource-tagging">Resource Tagging and AMIs</h4>
<p>For this project, assign the tag with <code>Key: Project</code> and <code>Value: 3.4</code> for all EC2, EMR and Redshift resources</p>
<table class="table">
<thead>
            <tr>
              <th>Type</th>
              <th>AMI</th>
              <th>Cluster / Instance Type</th>  
            </tr>
</thead>
          <tbody><tr>
            <td>Runner instance</td>
            <td>ami-c68ea0ae</td>
            <td>m1.small</td>
          </tr>
          <tr>
            <td>MySQL instance</td>
            <td>ami-407a5528</td>
            <td>m3.xlarge</td>
          </tr>
          <tr>
            <td>Hive Cluster</td>
            <td>NA</td>
            <td>1 master: m1.large 2 core: m3.xlarge</td>
          </tr>
          <tr>
            <td>Redshift Cluster</td>
            <td>NA</td>
            <td>2 node dw2.large cluster</td>
          </tr>
</tbody></table>
</div>
<div class="bs-callout bs-callout-danger">
<h4>Caution</h4>
<p>Some of the services including the MySQL instance, Hive cluster and Redshift cluster that you will be using for this project are expensive when left running. Please plan ahead and pay special attention to the instructions and tasks before provisioning any resources.</p>
</div>
</div>
<div class="bs-docs-section">
<h1 id="star" class="page-header">The Star Schema Benchmark Schema Design</h1>
<p>Data warehouse databases commonly use a <b>star schema</b> design. The star schema is the simplest data warehouse schema, so called because it’s representation resembles a star, with points radiating from a center. The center of the star consists of one or more fact tables and the points of the star are the dimension tables.</p>
<p>A star schema is characterized by one or more very large fact tables that contain the primary information in the data warehouse and a number of much smaller dimension tables (or lookup tables), each of which contains information about the entries for a particular attribute in the fact table.
</p>
<p>A <b>fact table</b> is at the center of a star schema. It has quantitative information about a business process that needs to analyzed. For example, a fact table may contain a record for a sales transaction. The record contains details such as sales <code>lo_quantity</code>, <code>lo_revenue</code>, <code>lo_orderdate</code>, <code>lo_custkey</code> etc. In the above example the data that needs to be analyzed is <code>lo_quantity</code> and <code>lo_revenue</code>. The <code>lo_orderdate</code> and the <code>lo_custkey</code> are foreign keys to dimension tables. The dimension tables store information about ways in which the fact table can be analyzed. For instance, the <code>lo_orderdate</code> can be used to filter information from the fact table to calculate the revenue from a particular month.</p>
<p>The following figure shows the SSB data model we will be using for the benchmark.</p>
<img src="./P3.4_files/3_4_ssb_data_model.png" width="90%" height="50%">
<h4>
  <small class="caption">
    <p>Figure 3: The SSB data model</p>
  </small>
</h4>
<p>A star query is a join between a fact table and a number of lookup tables. Each lookup table is joined to the fact table using a primary-key to foreign-key join, but the lookup tables are not joined to each other.</p>
<p><b>Definition</b></p>
<p>A <b>primary key</b>, also called a primary keyword, is a key in a relational database that is unique for each record. It is a unique identifier, such as a driver license number, telephone number (including area code), or vehicle identification number (VIN). A relational database must always have one and only one primary key.</p>
<p>A <b>foreign key</b> is a field (or collection of fields) in one table that uniquely identifies a row of another table. The foreign key in one table (referencing table) matches the primary key column of another table (referenced table). The foreign key can be used to cross-reference tables.</p>
<p>A <b>star join</b> is a primary-key to foreign-key join of the dimension tables to a fact table. All of the dimension tables have a primary key. The central fact table has a foreign key mapping to the dimension tables’ primary key.  A star join selects rows from the central fact table for aggregation based on a restriction (filtering condition) on the dimension tables.</p>
<p>The main advantages of star schemas are that they:</p>
<ol>
<li>Provide a direct and intuitive mapping between the business entities being analyzed by end users and the schema design. In an OLTP system, the tables are highly normalized. Normalization of tables is done to reduce data redundancy for efficient writes. The normalization of tables lead to more complex relationship between the entities (tables) in the database. A star schema reduces this complex relationship by flattening the relationship between tables by having more redundant data in the tables.</li>
<li>Provides highly optimized performance for typical data warehouse queries. This is because, the central fact table has significantly more number of rows than the dimension tables to which it is being joined. This enables hash joins between the fact table and the smaller dimension table. In a hash join, the central fact table is processed row-by-row, looking up the matching values from the in-memory hash tables of the smaller dimension tables. </li>
</ol>
<p>The main disadvantage of the star schema is that <a href="http://en.wikipedia.org/wiki/Data_integrity">data integrity</a> is not enforced as well as it is in a highly normalized database. One-off inserts and updates can result in data anomalies which <a href="http://en.wikipedia.org/wiki/Database_normalization">normalized</a> schemas are designed to avoid. Generally speaking, star schemas are loaded in a highly controlled fashion via batch processing or near-real time "trickle feeds", to compensate for the lack of protection afforded by normalization.</p>
<p>The following queries are part of the SSB benchmark. They capture a typical analytic query that will be performed in a data warehouse system. You will be running Query1 on MySQL, Hive and Redshift to asses the performance of each system for a typical data warehouse query. You will use Query 2 and Query 3 to asses the performance optimizations in Redshift possible due to its columnar and distributed storage.</p>
<p><b>Query1</b>: The first query in the benchmark will have a restriction (filtering condition) on only one dimension. This is a "what if" query to find possible revenue increases. The query is meant to quantify the amount of revenue increase that would have resulted from eliminating certain company-wide discounts in a given percentage range for products shipped in a given year. 
</p><pre>select sum(lo_extendedprice*lo_discount) as revenue from lineorder, dwdate where lo_orderdate=d_datekey and d_year=1997 and lo_discount between 1 and 3 and lo_quantity &lt; 24;</pre><p></p>
<p><b>Query2</b>: The second query will have restrictions on two dimensions. Our query will compare revenue for some product classes, for suppliers in a certain region, grouped by more restrictive product classes and all years of order.
</p><pre>select sum(lo_revenue), d_year, p_brand1 from lineorder, dwdate, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey and lo_suppkey = s_suppkey and p_category = 'MFGR#12' and s_region = 'AMERICA' group by d_year, p_brand1 order by d_year, p_brand1;
</pre><p></p>
<p><b>Query3</b>: The third query will have restrictions on three dimensions, including the remaining dimension, customer. The query is intended to provide revenue volume for <code>lineorder</code> transactions by customer nation and supplier nation and year within a given region, in a certain time period.</p><pre>select c_city, s_city, d_year, sum(lo_revenue) as revenue from customer, lineorder, supplier, dwdate where lo_custkey = c_custkey and lo_suppkey = s_suppkey and lo_orderdate = d_datekey and (c_city='UNITED KI1' or c_city='UNITED KI5') and (s_city='UNITED KI1' or s_city='UNITED KI5') and d_yearmonth = 'Dec1997' group by c_city, s_city, d_year order by d_year asc, revenue desc; 
</pre><p></p>
<div class="bs-docs-section">
<h1 id="warehouse" class="page-header">Introduction to MySQL, Hive and Redshift</h1>
<h3 id="mysql">MySQL</h3>
<p>As we have seen before MySQL is an open source row-store database. It is typically used for Online Transaction processing (OLTP) systems mainly because for OLTP systems, tuples are arranged in rows which are stored in blocks. The blocks reside on disk and are cached in main memory in the database server. This improves transaction latency in OLTP systems.</p>
<img src="./P3.4_files/3_4_block_level_storage.png" width="90%" height="50%">
<h4>
  <small class="caption">
    <p>Figure 4: Block level storage of rows in a row-store</p>
  </small>
</h4>
<p>In a typical relational database table, each row contains field values for a single record. In row-wise database storage, data blocks store values sequentially for each consecutive column making up the entire row. If block size is smaller than the size of a record, storage for an entire record may take more than one block. If block size is larger than the size of a record, storage for an entire record may take less than one block, resulting in an inefficient use of disk space. In online transaction processing (OLTP) applications, most transactions involve frequently reading and writing all of the values for entire records, typically one record or a small number of records at a time. As a result, row-wise storage is optimal for OLTP databases.</p>
<h3 id="hive">Hive</h3>
<p>The Apache Hive data warehouse software facilitates querying and managing large datasets residing in distributed storage (HDFS). Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called HiveQL. Programmers are also given the ability to plugin custom Mappers and Reducers to analyze data when such a solution is more efficient or it is not possible to automatically project structure onto the data in the distributed storage.</p>
<p>Hive is not designed for OLTP workloads and does not offer real-time queries or row-level updates. It is best used for batch jobs over large sets of append-only data (like web logs). The most important characteristics of Hive are:</p>
<ol>
<li>Scalability (scale out with more machines added dynamically to the Hadoop cluster).</li>
<li>Extensibility (with MapReduce framework).</li>
<li>fault-tolerance (provided by HDFS).</li>
<li>loose-coupling with its input formats (using SerDe (Serializer/Desirializer) explained below).</li>
</ol>
<p>The following figure gives a high level overview of Hive and its relationship with HDFS.</p>
<img src="./P3.4_files/3_4_hive_architecture.png" width="90%" height="50%">
<h4>
  <small class="caption">
    <p>Figure 5: Hive Architecture</p>
  </small>
</h4>
<p>The Hive server has a SQL parser, planner, executor and optimizer that will transform the SQL query to a MapReduce job that runs using the HDFS file system. The Hive metastore service stores the metadata for Hive tables and partitions in a relational database, and provides clients (including Hive) access to this information via the metastore service API. The metadata which metastore stores contains things like IDs of Database, IDs of Tables, IDs of Index, The time of creation of an Index, The time of creation of a Table, IDs of roles assigned to a particular user, InputFormat used for a Table OutputFormat used for a Table etc.</p>
<p>HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools — Pig, MapReduce — to more easily read and write data on the grid. HCatalog’s table abstraction presents users with a relational view of data in the Hadoop Distributed File System (HDFS) and ensures that users need not worry about where or in what format their data is stored — RCFile format, text files, SequenceFiles, or ORC files.</p>
<img src="./P3.4_files/3_4_HCatalog_layer.png" width="90%" height="50%">
<h4>
  <small class="caption">
    <p>Figure 6: The HCatalog layer</p>
  </small>
</h4>
<p>HCatalog supports reading and writing files in any format for which a SerDe (serializer-deserializer) can be written. By default, HCatalog supports RCFile, CSV, JSON, and SequenceFile, and ORC file formats. To use a custom format, you must provide the InputFormat, OutputFormat, and SerDe.</p>
<h3 id="redshift">Redshift</h3>
<p>An Amazon Redshift data warehouse is an enterprise-class relational database query and management system. Redshift is a cloud service that is offered by Amazon AWS. It is fully managed by AWS and it runs on top of EC2 with optimizations such as locally attached storage, high bandwidth interconnect across compute nodes etc. Redshift claims to support peta-byte scale data warehouse by increasing the number of compute nodes in the cluster. </p>
<p>The following figure and sections give a high level overview of Redshift’s architecture and components.</p>
<img src="./P3.4_files/3_4_redshift.png" width="90%" height="50%">
<h4>
  <small class="caption">
    <p>Figure 7: Redshift Data Warehouse architecture</p>
  </small>
</h4>
<p><b>Leader node</b></p>
<p>The leader node manages communications with client programs and all communication with compute nodes. It parses and develops execution plans to carry out database operations. An execution plan is the series of steps necessary to obtain results for a complex query. Based on the execution plan, the leader node compiles code, distributes the compiled code to the compute nodes, and assigns a portion of the data to each compute node.</p>
<p><b>Compute nodes</b></p>
<p>The leader node compiles code for individual elements of the execution plan and assigns the code to individual compute nodes. The compute nodes execute the compiled code send intermediate results back to the leader node for final aggregation. Each compute node has its own dedicated CPU, memory, and attached disk storage, which are determined by the node type.</p>
<p><b>Node slices</b></p>
<p>A compute node is partitioned into slices; one slice for each core of the node's multi-core processor. Each slice is allocated a portion of the node's memory and disk space, where it processes a portion of the workload assigned to the node. The leader node manages distributing data to the slices and assigns the workload for any queries or other database operations to the slices. The slices then work in parallel to complete the operation.</p>
<p>When you create a table, you can optionally specify one column as the distribution key. When the table is loaded with data, the rows are distributed to the node slices according to the distribution key that is defined for a table. Choosing a good distribution key enables Amazon Redshift to use parallel processing to load data and execute queries efficiently.</p>
<p>Using columnar storage, each data block stores values of a single column for multiple rows.</p>
<img src="./P3.4_files/3_4_block_level_storage_columns.png" width="90%" height="50%">
<h4>
  <small class="caption">
    <p>Figure 8: Block level storage of columns in column store</p>
  </small>
</h4>
<p>Using columnar storage, each data block holds column field values for as many as three times as many records as row-based storage. This means that reading the same number of column field values for the same number of records requires a third of the I/O operations compared to row-wise storage. Since each block holds the same type of data, block data can use a compression scheme selected specifically for the column data type, further reducing disk space and I/O. </p>
<p>The savings in space for storing data on disk also carries over to retrieving and then storing that data in memory. Since many database operations only need to access or operate on one or a small number of columns at a time, you can save memory space by only retrieving blocks for columns you actually need for a query. Where OLTP transactions typically involve most or all of the columns in a row for a small number of records, data warehouse queries commonly read only a few columns for a very large number of rows. This means that reading the same number of column field values for the same number of rows requires a fraction of the I/O operations and uses a fraction of the memory that would be required for processing row-wise blocks.</p>
<div class="bs-docs-section">
<h1 id="bench" class="page-header">Benchmarking</h1>
<p>In order to run the sql queries on the backend instance, we have provided you with a runner program and SQL script. The details of the SQL script and the runner program are given below.</p>
<p><b>The “project3_4.sql” script</b></p>
<p>This is a SQL script that can be found in the runner instance AMI (<code>ami-c68ea0ae</code>) at the path <code>/home/ubuntu/Project3_4/project3_4.sql</code>. The runner program will read SQL commands from this script and run it in the specified backend (MySQL/Hive/Redshift).
The script is divided into multiple sections. Each section can have multiple SQL commands delimited by <code>;</code>.
Each section is identified by the following start and end comments:
</p><pre>-- start section_name
sql commands delimited with ;
-- end section_name</pre><p></p>
<div class="bs-callout bs-callout-info">
<h4>Note</h4>
<p>Do not remove any existing sections and commands from the <code>project3_4.sql</code> script as the auto-grader relies on the section names to report scores to the auto grader. You may modify the section <code>redshift_create_table_optimized</code> to select the sort and dist keys. You may add additional sections for your own curiosity. </p>
</div>
<p><b>The Runner program</b></p>
<p>The Runner program is located at <code>/home/ubuntu/Project3_4/Runner.jar</code>. You can use the runner program to run the required section from the <code>project3_4.sql</code> on the specified backend (MySQL/Hive/Redshift).</p>
<p>The Runner program has to be configured using the <code>config.properties</code> file located at <code>/home/ubuntu/Project3_4/</code>. </p>
<p>The <code>config.properties</code> file has the following structure.
</p><pre>REDSHIFT_JDBC_URL= redshift jdbc endpoint
REDSHIFT_USERNAME= username
REDSHIFT_PASSWORD= password

MYSQL_JDBC_URL=jdbc:mysql://MySQL instance DNS:3306/ssb
MYSQL_USERNAME=root
MYSQL_PASSWORD=db15319root

HIVE_JDBC_URL=jdbc:hive2://Hive(EMR) cluster master DNS:10000
</pre>
<p></p>
<p>The Runner program can be run as follows,</p>
<p></p><pre>Usage: java -jar Runner.jar &lt;backend&gt; &lt;section_name&gt;
backend : “mysql” or “hive” or “redshift”
section_name : section name to run from the <code>project3_4.sql</code>
</pre><p></p>
<div class="bs-callout bs-callout-info">
<h4>Note</h4>
<p>You will be graded for this project only if you run the benchmark queries on all three databases using the runner script. </p>
</div>
<div class="bs-callout bs-callout-danger">
<h4>Caution</h4>
<p>Do not run multiple processes of the Runner program from the same instance at the same time. If you would like to work in parallel please start a new Runner instance.</p>
</div>
<p>The MySQL Task is designed to help you compare row-store based OLTP systems with distributed column-store based OLAP systems. The Hive Task helps you analyze the advantages and disadvantages of using MapReduce programming for data warehousing workloads. The Redshift task is designed to help you explore the advantages of distributed columnar storage for OLAP workloads and optimization techniques available in distributed column-store using sort keys and dist keys.</p>
<h3 id="mysql_task">MySQL Task</h3>
<p>We will now benchmark the performance of ssb query1 in a MySQL instance. For this task, we will be launching an ami (<code>ami-407a5528</code>) on a <code>m3.xlarge</code> instance. This instance has the ssb dataset loaded and indexed. </p>
<div class="bs-callout bs-callout-info">
<h4>Note</h4>
<p>The following credentials are used to access the database </p>
<p><b>username</b>: <code>root</code></p>
<p><b>password</b>: <code>db15319root</code></p>
<p><b>database</b>: <code>ssb</code></p>
<p>Make sure that the instance’s security group allows traffic to port 3306 for the JDBC connection</p>
</div>
<div class="bs-callout bs-callout-task">
<h4>Steps</h4>
<p>To run the benchmark for this experiment, perform the following steps:</p>
<ol>
<li>Update the <code>config.properties</code> file in <code>/home/ubuntu/Project3_4/</code> with the correct DNS, username, password and DNS name before running the runner script.</li>
<li>Login to the instance and the MySQL shell.</li>
<li>Determine the total size (Data_length) occupied by all tables in the ssb database.</li>
<li>Determine the cardinality of indexes created on the tables <code>lineorder</code> and <code>dwdate</code>.</li>
<li>Determine the buffer pool size allocated for MySQL.</li>
<li>Run the explain command on the ssb query1. Determine the indexes that are being used in the query</li>
<li>Run ssb query1 on MySQL using the Runner in the Runner instance. Execute section <code>query1</code> from <code>project3_4.sql</code> for dbType MySQL. Run the query twice one after another. Do you notice a difference?</li>
</ol>
</div>
<div class="bs-callout bs-callout-danger">
<h4>Caution</h4>
<p>The query takes a significant amount of time to run. Please be patient. Please use either <code>byobu</code> or <code>screen</code> utilities to protect your Runner process from terminating. If your Runner process terminates before the query runs, the scores will not be reported to the auto-grading server</p>
</div>
<h3 id="hive_task">Hive Task</h3>
<div class="bs-callout bs-callout-task">
<h4>Steps</h4>
<p></p><p><b>Launch a EMR cluster with Hive</b></p><p></p>
<p>We will now benchmark the performance of ssb-query1 on a Hive cluster to analyze the advantages and disadvantages of MapReduce for data warehousing. For this task we will be using a 1 master 2 core EMR cluster with Hive application installed. The following is the configuration you will be using for the EMR cluster.</p>
<p></p><pre>Cluster Name: EMR Cluster Name
Termination Protection: Yes
Tags: Key: Project Value: 3.4
AMI Version: 3.5.0
Applications to be installed: Hive 0.13.1
Hardware Configuration: Master m1.large 1 and Core m3.xlarge 2
Ec2 Keypair: your ec2 key pair
</pre><p></p>
<p>Make sure that the cluster's security group allows traffic to port 10000 for the JDBC connection</p>
<ol>
<li>Once the Cluster is running, use the master instance’s DNS in the jdbc endpoint for the runner program. You can set the value in <code>config.properties</code> file.</li>
<li>Execute the <code>hive_create_table</code> section in <code>project3_4.sql</code> using the runner program.</li>
<li>Execute the <code>query1</code> section in <code>project3_4.sql</code> using the runner program</li>
</ol>
</div>
<div class="bs-callout bs-callout-info">
<h4>Note</h4>
<p>The data for the ssb dataset is located at <code>s3://15619s15p34/ssbgz/</code>. The Hive queries will automatically fetch data from this location to run the MapReduce job.</p>
</div>
<h3 id="redshift_task">Redshift Task</h3>
<div class="bs-callout bs-callout-task">
<h4>Launch a Redshift Cluster</h4>
<p>For this task, we will be launching a multi-node redshift cluster with 1 leader (provided for free) and 2 compute nodes (<code>dw2.large</code> @$0.25/hr). Please follow the steps below for launching the cluster.</p>
<ol>
<li>Sign in to the AWS Management Console and open the Amazon Redshift console at <a href="https://console.aws.amazon.com/redshift/">here</a>.</li>
<li>In the left navigation pane, click Clusters and then click Launch Cluster.</li>
<li>On the Cluster Details page, enter the following values and then click Continue:
<ol>
<li><b>Cluster Identifier</b>: Type an identifier of your choice. (example: ssb-benchmark)</li>
<li><b>Database Name</b>: Type a database name of your choice. (example: ssb)</li>
<li><b>Database Port</b>: Leave the port number to the default value. Make sure that the security group you select in the subsequent step allows traffic on this port</li>
<li><b>Master User Name</b>: Type a master user name of your choice. You will use this username and password to connect to your database after the cluster is available.</li>
<li><b>Master User Password and Confirm Password</b>: Type a password for the master user account. Make sure you remember the user name and password. You will use these values to establish a JDBC connection to the redshift cluster.</li>
</ol>
</li>
<li>On the Node Configuration page, select the following values and click continue
<ol>
<li><b>Node Type</b>: dw2.large</li>
<li><b>Cluster Type</b>: Multi Node</li>
<li><b>Number of Compute Nodes</b>: 2</li>
</ol>
</li>
<li>On the Additional Configuration page
<ol>
<li><b>Cluster Parameter Group</b>: select the default parameter group.</li>
<li><b>Encrypt Database</b>: None</li>
<li><b>Choose a VPC</b>: Default VPC (vpc-xxxxxxxx)</li>
<li><b>Cluster Subnet Group</b>: default</li>
<li><b>Publicly Accessible</b>: Yes</li>
<li><b>Choose a Public IP Address</b>: No</li>
<li><b>Availability Zone</b>: No Preference</li>
<li><b>VPC Security Groups</b>: Select a security group that allows traffic on the database port you selected in step 3</li>
<li><b>Create CloudWatch Alarm</b>: No</li>
</ol>
</li>
<li>On the Review page, review the selections that you have made and then click Launch Cluster
<img src="./P3.4_files/3_4_redshift_review_page.png" width="90%" height="50%">
<h4>
  <small class="caption">
    <p>screenshot 1</p>
  </small>
</h4>
</li>
<li>On the Clusters page, click the cluster that you just launched and review the Cluster Status information. Make sure that the Cluster Status is available and the Database Health is healthy before you try to connect to the database
<img src="./P3.4_files/3_4_cluster_status_info.png" width="90%" height="50%">
<h4>
  <small class="caption">
    <p>screenshot 2</p>
  </small>
</h4>
</li>
<li>Copy the username, password and the jdbc endpoint url to the file <code>“/home/ubuntu/Project3_4/config.properties”</code></li>
</ol>
</div>
<p>We have launched the cluster and configured the runner program to connect to our redshift cluster. We will now proceed to benchmark query performance on redshift for the ssb-benchmark dataset</p>
<p>The following steps, (Steps 1-4) is used to analyze the optimizations possible in Redshift due to its columnar distributed storage. In Steps 1 and 2 we will test the performance of Redshift cluster without applying the optimizations. In Step 3 we will explore the capabilities and optimizations possible in a Redshift cluster. Finally in Step 4 we will apply the optimizations discussed in Step 3 and benchmark the performance of ssb queries to evaluate any improvements in performance.</p>
<div class="bs-callout bs-callout-task">
<h4>Step 1: Create the test dataset</h4>
<ol>
<li>First we will create the ssb tables with minimum attributes (they will not have sort keys, distribution styles, or compression encodings).
Execute the “redshift_create_table_unoptimized” section from <code>project3_4.sql</code> using the runner program.</li>
<li>Load the tables using the ssb sample data.
Execute the “redshift_load_uncompressed” section from <code>project3_4.sql</code> using the runner program. Replace the <code>accesskey</code> and <code>secretkey</code> in the sql command with your aws access key and secret key respectively.
The copy command used in the above script will not apply automatic compression encodings to the columns in the table. This is done using the <code>gzip compupdate off</code> option in the copy command.</li>
<li>To verify the tables were loaded correctly, execute the “count_tables” section from <code>project3_4.sql</code> using the runner program.
The following results table shows the number of rows for each SSB table.
<table class="table">
          <thead>
            <tr>
              <th>Table Name</th>
              <th>Rows</th>
            </tr>
          </thead>
          <tbody><tr>
            <td>LINEORDER</td>
            <td>600,037,902</td>
          </tr>
          <tr>
            <td>PART</td>
            <td>1,400,000</td>
          </tr>
          <tr>
            <td>CUSTOMER</td>
            <td>3,000,000</td>
          </tr>
          <tr>
            <td>SUPPLIER</td>
            <td>1,000,000</td>
          </tr>
          <tr>
            <td>DWDATE</td>
            <td>2,556</td>
          </tr>
        </tbody></table>
</li>
</ol>
</div>
<div class="bs-callout bs-callout-task">
<h4>Step 2: Test System Performance to Establish a Baseline</h4>
<ol>
<li>Note the cumulative load time for all five tables. (Step 1 Point 2)</li>
<li>Record storage use. Determine how many 1 MB blocks of disk space are used for each table by querying the <code>STV_BLOCKLIST</code> table.
Execute the <code>redshift_table_space</code> section from <code>project3_4.sql</code> using the runner program.</li>
<li>Test query performance. The first time you run a query, Amazon Redshift compiles the code, and then sends compiled code to the compute nodes. When you compare the execution times for queries, you should not use the results for the first time you execute the query. Instead, compare the times for the second execution of each query.
Execute the <code>query1</code>, <code>query2</code>, <code>query3</code> sections from <code>project3_4.sql</code> using the runner program.</li>
</ol>
</div>
<div class="bs-callout bs-callout-task">
<h4 id="step3">Step 3: Optimizing table design for query performance. Choosing sort and dist keys</h4>
</div>
<p><b>Sort Keys:</b></p>
<p>When you create a table, you can specify one or more columns as the sort key. Amazon Redshift stores your data on disk in sorted order according to the sort key. How your data is sorted has an important effect on disk I/O, columnar compression, and query performance.
In this step, you choose sort keys for the SSB tables based on these best practices:
</p><ol>
<li>If recent data is queried most frequently, specify the timestamp column as the leading column for the sort key.</li>
<li>If you do frequent range filtering or equality filtering on one column, specify that column as the sort key.</li>
<li>If you frequently join a (dimension) table, specify the join column as the sort key.</li>
</ol>
<p></p>
<div class="bs-callout bs-callout-task">
<h4>To select sort keys:</h4>
<ol>
<li>Evaluate your queries to find timestamp columns that are used to filter the results.</li>
<li>Look for columns that are used in range filters and equality filters.</li>
<li>For smaller dimension tables, choose the primary keys as their sort keys.</li>
</ol>
</div>
<p><b>Dist Keys:</b></p>
<p>When you load data into a table, Amazon Redshift distributes the rows of the table to each of the node slices according to the table's distribution style. The number of slices is equal to the number of processor cores on the node. For example, the <code>dw2.large</code> cluster that you are using in this tutorial has four nodes, so it has eight slices. The nodes all participate in parallel query execution, working on data that is distributed across the slices.</p>
<p>When you execute a query, the query optimizer redistributes the rows to the compute nodes as needed to perform any joins and aggregations. Redistribution might involve either sending specific rows to nodes for joining or broadcasting an entire table to all of the nodes.</p>
<p>You should assign distribution styles to achieve these goals:
</p><ol>
<li>Collocate the rows from joining tables</li>
<li>When the rows for joining columns are on the same slices, less data needs to be moved during query execution.</li>
<li>Distribute data evenly among the slices in a cluster.</li>
<li>If data is distributed evenly, the workload can be allocated evenly to all the slices.</li>
</ol>
<p></p>
<p>These goals may conflict in some cases, and you will need to evaluate which strategy is the best choice for overall system performance. For example, even distribution might place all matching values for a column on the same slice. If a query uses an equality filter on that column, the slice with those values will carry a disproportionate share of the workload. If tables are collocated based on a distribution key, the rows might be distributed unevenly to the slices because the keys are distributed unevenly through the table.</p>
<p>In this step, you evaluate the distribution of the SSB tables with respect to the goals of data distribution, and then select the optimum distribution styles for the tables.</p>
<p><b>Distribution Styles</b></p>
<p>When you create a table, you designate one of three distribution styles: <code>KEY</code>, <code>ALL</code>, or <code>EVEN</code>.
</p><ol>
<li><b>KEY distribution</b>:The rows are distributed according to the values in one column. The leader node will attempt to place matching values on the same node slice. If you distribute a pair of tables on the joining keys, the leader node collocates the rows on the slices according to the values in the joining columns so that matching values from the common columns are physically stored together.</li>
<li><b>ALL distribution</b>:A copy of the entire table is distributed to every node. Where <code>EVEN</code> distribution or <code>KEY</code> distribution place only a portion of a table's rows on each node, ALL distribution ensures that every row is collocated for every join that the table participates in.</li>
<li><b>EVEN distribution</b>:The rows are distributed across the slices in a round-robin fashion, regardless of the values in any particular column. <code>EVEN</code> distribution is appropriate when a table does not participate in joins or when there is not a clear choice between <code>KEY</code> distribution and <code>ALL</code> distribution. <code>EVEN</code> distribution is the default distribution style.</li>
</ol>
<p></p>
<div class="bs-callout bs-callout-task">
<h4>To Select Distribution Styles:</h4>
<p>When you execute a query, the query optimizer redistributes the rows to the compute nodes as needed to perform any joins and aggregations. By locating the data where it needs to be before the query is executed, you can minimize the impact of the redistribution step.</p>
<p>The first goal is to distribute the data so that the matching rows from joining tables are collocated, which means that the matching rows from joining tables are located on the same node slice.</p>
<ol>
<li>To look for redistribution steps in the query plan, execute an <code>EXPLAIN</code> command followed by the query.
Execute the <code>explain_query2</code> section from <code>project3_4.sql</code> using the runner program.
<code>DS_BCAST_INNER</code> indicates that the inner join table was broadcast to every slice. A <code>DS_DIST_BOTH</code> label, if present, would indicate that both the outer join table and the inner join table were redistributed across the slices. Broadcasting and redistribution can be expensive steps in terms of query performance. You want to select distribution strategies that reduce or eliminate broadcast and distribution steps.</li>
<li>Distribute the fact table and one dimension table on their common columns. Each table can have only one distribution key, which means that only one pair of tables in the schema can be collocated on their common columns. The central fact table is the clear first choice. For the second table in the pair, choose the largest dimension that commonly joins the fact table by analyzing the output from step 1.</li>
</ol>
</div>
<div class="bs-callout bs-callout-info">
<h4>Hint</h4>
<p>1. Query 1 has equality filtering on the <code>lineorder</code> table for one of its field. That should be the sort key for <code>lineorder</code>. All other dimension tables are small when compared to <code>lineorder</code> and hence according to best practice we can sort them based on primary key. </p>
<p>2. Only 2 tables can be collocated with each other based on the join column. Clearly, we have to collocate one of the tables with <code>lineorder</code> based on one join column from <code>lineorder</code> and the other table. Identify the other table from the analyze command for Query 2. Use the table which is larger among the tables you find in this query.
</p>
<p>3. Run the analyze command for Query2 again before loading the data. Make sure you do not see <code>DS_BCAST_INNER</code> join before loading the data. This will save you some time by making sure your optimization works without waiting for the data to load.</p>
</div>
<div class="bs-callout bs-callout-task">
<h4>Step 4 Recreate test dataset and benchmark queries</h4>
<ol>
<li>You need to drop the SSB tables before you run the <code>CREATE TABLE</code> commands. Execute the <code>drop_tables</code> section from <code>project3_4.sql</code> using the runner program.</li>
<li><p>Create the tables with sort keys and distribution styles.</p>
<p>Execute the “redshift_create_table_optimized” section from <code>project3_4.sql</code> using the runner program. Change the create table DDL for all tables so that the right <code>sortkey</code> and <code>distkey</code> are selected. </p>
<p>(Note: If a <code>distkey</code> is selected, the distribution style is <code>KEY</code> distribution. If <code>diststyle</code> all is specified, it is <code>ALL</code> distribution. The default is <code>EVEN</code> distribution)</p>
<p>For example, consider the <code>dwdate</code> table. This is a dimension table, so the best sort key is the primary key of the table. We could also distribute the table across the nodes with the same field.</p>
<p>The way to create the table with both <code>sortkey</code> and <code>distkey</code> set to the primary key of <code>dwdate</code> table would be</p>
<pre>CREATE TABLE dwdate (
  d_datekey         integer     not null sortkey distkey,
  d_date            varchar(19)   not null,
  d_dayofweek       varchar(10)   not null,
  d_month           varchar(10)   not null,
  d_year            integer     not null,
  d_yearmonthnum    integer       not null,
  d_yearmonth       varchar(8)    not null,
  d_daynuminweek    integer     not null,
  d_daynuminmonth   integer     not null,
  d_daynuminyear    integer     not null,
  d_monthnuminyear  integer     not null,
  d_weeknuminyear   integer     not null,
  d_sellingseason   varchar(13) not null,
  d_lastdayinweekfl varchar(1)  not null,
  d_lastdayinmonthfl   varchar(1)   not null,
  d_holidayfl       varchar(1)  not null,
  d_weekdayfl       varchar(1)  not null
);
</pre>
<p>But this is a small dimension table, this table can be replicated in all nodes by occupying some additional space on all nodes in the cluster. i.e. <code>diststyle all</code>. So the optimized table design for dwdate would be</p>
<pre>CREATE TABLE dwdate (
  d_datekey         integer     not null sortkey,
  d_date            varchar(19)   not null,
  d_dayofweek       varchar(10)   not null,
  d_month           varchar(10)   not null,
  d_year            integer     not null,
  d_yearmonthnum    integer       not null,
  d_yearmonth       varchar(8)    not null,
  d_daynuminweek    integer     not null,
  d_daynuminmonth   integer     not null,
  d_daynuminyear    integer     not null,
  d_monthnuminyear  integer     not null,
  d_weeknuminyear   integer     not null,
  d_sellingseason   varchar(13) not null,
  d_lastdayinweekfl varchar(1)  not null,
  d_lastdayinmonthfl   varchar(1)   not null,
  d_holidayfl       varchar(1)  not null,
  d_weekdayfl       varchar(1)  not null)
diststyle all; 
</pre>
</li>
<li>Load the tables using the same sample data. This time we will make use of copy commands auto compress feature to automatically analyze and apply appropriate compression schemes for the columns.
Execute the <code>redshift_load_compressed</code> section from <code>project3_4.sql</code> using the runner program.</li>
<li>Test query performance. The first time you run a query, Amazon Redshift compiles the code, and then sends compiled code to the compute nodes. When you compare the execution times for queries, you should not use the results for the first time you execute the query. Instead, compare the times for the second execution of each query. Execute the “query1”, “query2”, “query3” sections from <code>project3_4.sql</code> using the runner program.</li>
<li>Analyze the compression schemes employed for each column in the lineorder table. Execute the “analyze_compression” section from <code>project3_4.sql</code> using the runner program.</li>
</ol>
</div>
<p>Complete and submit the quiz using <code>runner.sh</code> and submitter from the folder <code>/home/ubuntu/Project3_4/</code> to complete the project.</p>
<div class="bs-docs-section">
<h1 id="grading" class="page-header">Grading</h1>
<p>To complete the project, after completing the tasks above, you are expected to answer some questions provided in the Runner AMI. The quiz questions are present in the file <code>/home/ubuntu/Project3_4/runner.sh</code>. You can verify and submit your results using the given auto-grader in the AMI. The quiz questions are designed to test your understanding of the advantages and disadvantages of each of the data warehousing solutions you have benchmarked. To use the autograder, do the following:</p>
<ol>
<li>
<p>Go to the auto-grader folder located at <code>/home/ubuntu/Project3_4/</code></p>
</li>
<li>
<p>The auto-grader consists of four files, <code>Runner.jar</code>, <code>runner.sh</code>, <code>submitter</code> and <code>references</code>. You have permissions to edit <code>runner.sh</code> and <code>references</code> files.</p>
</li>
<li>
<p>Edit the script <code>runner.sh</code> to include the answers to the quiz questions.</p>
</li>
<li>
<p>Edit the text file <code>references</code> to include all the links that you referred to for completing this project. Remember, copying any code from the internet is considered cheating. Also include the Andrew IDs of all the other students who you might have discussed general ideas with when working on this project in the same file. Please remember, copying any code from any other student is considered cheating.</p>
</li>
<li>
<p>You can run the autograder by typing <code>./runner.sh</code> from the autograder folder. Running this script should print out the answers to all the questions. Please ensure that the answers are printing correctly before using submitter.</p>
</li>
<li>
<p>Once you have completed all the questions, you can submit the answers to the evaluation system using the auto-grader executable submitter. Run the executable using the command <code>./submitter</code> from the autograder folder. After running this command, you should be able to see your scores on the website in a few minutes. <b>You are limited to 3 quiz submissions. So decide carefully before submitting the results.</b> Each submission must be separated by at least 60 seconds.</p>
</li>
</ol>
<div class="bs-callout bs-callout-danger">
<h4>Project Grading Penalties</h4>
<p>Besides the penalties mentioned in recitation and/or on Piazza, penalties accrue for the following:</p>

        <table class="table">
          <thead>
            <tr>
              <th>Violation</th>
              <th>Penalty of the project grade</th>
            </tr>
          </thead>
          <tbody><tr>
            <td>Spending more than $15 for this project checkpoint</td>
            <td>-10%</td>
          </tr>
          <tr>
            <td>Spending more than $30 for this project checkpoint</td>
            <td>-100%</td>
          </tr>
          <tr>
            <td>Failing to tag all your resources for this project</td>
            <td>-10%</td>
          </tr>
          <tr>
            <td>Using any instance other than the ones specified in the writeup</td>
            <td>-10%</td>
          </tr>
          <tr>
            <td>Attempting to hack/tamper the auto-grader</td>
            <td>-100%</td>
          </tr>
        </tbody></table>
      </div>

<p></p></div>
</div>
<!-- JS Scripts -->
<script src="./P3.4_files/docs.min.js"></script>
<p></p>


    </div>


    </div>

    <div id="footer" class="navbar-fixed-bottom" style="height:30px;background-color:#f5f5f5;position:fixed;bottom:0;width:100%;text-align:center;padding:5px">
        <p class="text-muted credit">©2015 Teaching Staff Of The Cloud Computing Course, Carnegie Mellon University</p>
    </div>

    
    
    

    <script type="text/javascript">
        function dashIfUNE(str) {
            return (str==null || str.length===0)?'<span class="glyphicon glyphicon-minus"></span>':str;
        }
        /* https://docs.djangoproject.com/en/dev/ref/csrf/ */
        function csrfSafeMethod(method) {
            return (/^(GET|HEAD|OPTIONS|TRACE)$/.test(method));
        }
        $.ajaxSetup({
            beforeSend: function(xhr, settings) {
                if (!csrfSafeMethod(settings.type) && !this.crossDomain) {
                    xhr.setRequestHeader("X-CSRFToken", csrftoken);
                }
            }
        });
        function getCookie(name) {
            var cookieValue = null;
            if (document.cookie && document.cookie != '') {
                var cookies = document.cookie.split(';');
                for (var i = 0; i < cookies.length; i++) {
                    var cookie = jQuery.trim(cookies[i]);
                    if (cookie.substring(0, name.length + 1) == (name + '=')) {
                        cookieValue = decodeURIComponent(cookie.substring(name.length + 1));
                        break;
                    }
                }
            }
            return cookieValue;
        }
        var csrftoken = getCookie('csrftoken');
    </script>
    
    <script type="text/javascript">
        function fixNavbarTop() {
            $('#base-container').css('padding-top', $('.navbar-fixed-top').height()+10);
        }
        $(document).ready(fixNavbarTop);
        $(window).resize(fixNavbarTop);
        $(window).load(fixNavbarTop);
    </script>

    <script type="text/javascript">
        $(document).ready(function(){
            $('#nav_writeup').addClass('btn-primary');
            $("#project3").addClass('active');
        });
    </script>



</div></div></div></div></body></html>